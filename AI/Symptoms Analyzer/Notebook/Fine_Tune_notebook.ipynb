{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10847011,"sourceType":"datasetVersion","datasetId":6736519}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:24:33.721749Z","iopub.execute_input":"2025-02-25T09:24:33.722079Z","iopub.status.idle":"2025-02-25T09:24:34.058297Z","shell.execute_reply.started":"2025-02-25T09:24:33.722046Z","shell.execute_reply":"2025-02-25T09:24:34.057407Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/synthetic-medical-data/synthetic_medical_data.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"file_path = \"/kaggle/input/synthetic-medical-data/synthetic_medical_data.csv\"  \ndf = pd.read_csv(file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:24:34.059172Z","iopub.execute_input":"2025-02-25T09:24:34.059586Z","iopub.status.idle":"2025-02-25T09:24:34.185930Z","shell.execute_reply.started":"2025-02-25T09:24:34.059564Z","shell.execute_reply":"2025-02-25T09:24:34.185317Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df = df[[\"Generated Description\", \"Predicted Disease\"]]\ndf.rename(columns={\"Generated Description\": \"text\", \"Predicted Disease\": \"label\"}, inplace=True)\n\n# Display a few rows\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:24:34.186657Z","iopub.execute_input":"2025-02-25T09:24:34.186925Z","iopub.status.idle":"2025-02-25T09:24:34.210441Z","shell.execute_reply.started":"2025-02-25T09:24:34.186898Z","shell.execute_reply":"2025-02-25T09:24:34.209444Z"}},"outputs":[{"name":"stdout","text":"                                                text                label\n0  For the past few days, I have patches_in_throa...                 AIDS\n1  I noticed scurring, blackheads, skin_rash, pus...                 Acne\n2  I noticed yellowish_skin, abdominal_pain, dist...  Alcoholic Hepatitis\n3  For the past few days, I have chills, shiverin...              Allergy\n4  I have muscle_weakness, swelling_joints, painf...            Arthritis\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Encode disease labels\nlabel_encoder = LabelEncoder()\ndf[\"label\"] = label_encoder.fit_transform(df[\"label\"])\n\nimport pickle\n\n# Save the encoder\nwith open(\"label_encoder.pkl\", \"wb\") as f:\n    pickle.dump(label_encoder, f)\n\n\n# Save the label mapping for later decoding\nlabel_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:24:34.211227Z","iopub.execute_input":"2025-02-25T09:24:34.211503Z","iopub.status.idle":"2025-02-25T09:24:34.655875Z","shell.execute_reply.started":"2025-02-25T09:24:34.211477Z","shell.execute_reply":"2025-02-25T09:24:34.655179Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load PubMedBERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n\n# Tokenize the text descriptions\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n# Apply tokenization\nfrom datasets import Dataset\ndataset = Dataset.from_pandas(df)\ndataset = dataset.map(tokenize_function, batched=True)\ndataset = dataset.train_test_split(test_size=0.2)\n\n# Split into train and test sets\ntrain_dataset = dataset[\"train\"]\ntest_dataset = dataset[\"test\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:24:34.656832Z","iopub.execute_input":"2025-02-25T09:24:34.657237Z","iopub.status.idle":"2025-02-25T09:24:46.208774Z","shell.execute_reply.started":"2025-02-25T09:24:34.657213Z","shell.execute_reply":"2025-02-25T09:24:46.207923Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4475664116b94d28bb6ba41e3165bfbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8b77190803d4318af9eb06ad77815e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/225k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5bf9e45d66c454d92f2ee136cd8c4ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15e7d6a060d64093b6dc52c557348205"}},"metadata":{}},{"name":"stderr","text":"Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nnum_labels = len(label_mapping)  # Number of unique diseases\n\n# Load PubMedBERT model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n    num_labels=num_labels\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:24:46.209610Z","iopub.execute_input":"2025-02-25T09:24:46.209921Z","iopub.status.idle":"2025-02-25T09:25:01.278592Z","shell.execute_reply.started":"2025-02-25T09:24:46.209891Z","shell.execute_reply":"2025-02-25T09:25:01.277616Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b05a771fdb254e27ac62a2d3ae06014b"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./pubmedbert_disease_prediction\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    learning_rate= 2.4432297149345087e-05,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    save_total_limit=2,\n    load_best_model_at_end=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:25:01.280258Z","iopub.execute_input":"2025-02-25T09:25:01.281045Z","iopub.status.idle":"2025-02-25T09:25:02.619334Z","shell.execute_reply.started":"2025-02-25T09:25:01.281006Z","shell.execute_reply":"2025-02-25T09:25:02.618418Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!wandb login 2161a2b86735ea110585354d6ed05d7936b6e8ea","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:25:02.620275Z","iopub.execute_input":"2025-02-25T09:25:02.620598Z","iopub.status.idle":"2025-02-25T09:25:04.849496Z","shell.execute_reply.started":"2025-02-25T09:25:02.620568Z","shell.execute_reply":"2025-02-25T09:25:04.848518Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\n# Define the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer\n)\n\n# Start training\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T09:26:19.057070Z","iopub.execute_input":"2025-02-25T09:26:19.057370Z","iopub.status.idle":"2025-02-25T09:54:26.882899Z","shell.execute_reply.started":"2025-02-25T09:26:19.057348Z","shell.execute_reply":"2025-02-25T09:54:26.882206Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-10-6dca5a395642>:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdhrupalpatidar1313\u001b[0m (\u001b[33mdhrupalpatidar1313-wappnet-systems-pvt-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250225_092627-yxlcqn81</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dhrupalpatidar1313-wappnet-systems-pvt-/huggingface/runs/yxlcqn81' target=\"_blank\">./pubmedbert_disease_prediction</a></strong> to <a href='https://wandb.ai/dhrupalpatidar1313-wappnet-systems-pvt-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dhrupalpatidar1313-wappnet-systems-pvt-/huggingface' target=\"_blank\">https://wandb.ai/dhrupalpatidar1313-wappnet-systems-pvt-/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dhrupalpatidar1313-wappnet-systems-pvt-/huggingface/runs/yxlcqn81' target=\"_blank\">https://wandb.ai/dhrupalpatidar1313-wappnet-systems-pvt-/huggingface/runs/yxlcqn81</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5000/5000 27:51, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>5.761800</td>\n      <td>4.450728</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.848400</td>\n      <td>2.776561</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.470500</td>\n      <td>1.623545</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.465100</td>\n      <td>0.868883</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.820700</td>\n      <td>0.447445</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.458800</td>\n      <td>0.241959</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.271500</td>\n      <td>0.146925</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.180100</td>\n      <td>0.101535</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.134400</td>\n      <td>0.080753</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.115700</td>\n      <td>0.074485</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5000, training_loss=1.5527047676086425, metrics={'train_runtime': 1685.8003, 'train_samples_per_second': 94.91, 'train_steps_per_second': 2.966, 'total_flos': 4933122841598976.0, 'train_loss': 1.5527047676086425, 'epoch': 10.0})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"metrics = trainer.evaluate()\nprint(metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T10:01:04.649042Z","iopub.execute_input":"2025-02-25T10:01:04.649342Z","iopub.status.idle":"2025-02-25T10:01:15.719375Z","shell.execute_reply.started":"2025-02-25T10:01:04.649320Z","shell.execute_reply":"2025-02-25T10:01:15.718728Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 00:10]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.07448519021272659, 'eval_runtime': 11.0599, 'eval_samples_per_second': 361.667, 'eval_steps_per_second': 11.302, 'epoch': 10.0}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"model.save_pretrained(\"fine_tuned_pubmedbert\")\ntokenizer.save_pretrained(\"fine_tuned_pubmedbert\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T10:01:32.711501Z","iopub.execute_input":"2025-02-25T10:01:32.711834Z","iopub.status.idle":"2025-02-25T10:01:33.561559Z","shell.execute_reply.started":"2025-02-25T10:01:32.711806Z","shell.execute_reply":"2025-02-25T10:01:33.560832Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"('fine_tuned_pubmedbert/tokenizer_config.json',\n 'fine_tuned_pubmedbert/special_tokens_map.json',\n 'fine_tuned_pubmedbert/vocab.txt',\n 'fine_tuned_pubmedbert/added_tokens.json',\n 'fine_tuned_pubmedbert/tokenizer.json')"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from transformers import pipeline\n\nMODEL_PATH = \"/kaggle/working/fine_tuned_pubmedbert\"  # Change this if your model is saved elsewhere\n\n# Load the fine-tuned model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\nclassifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\ntext = \"I have a persistent cough, fever, and chills.\"\nresult = classifier(text)\n# Extract the numerical label from \"LABEL_X\"\npredicted_label_index = int(result[0][\"label\"].replace(\"LABEL_\", \"\"))  # Convert \"LABEL_476\" -> 476\n\n# Convert back to the actual disease name using LabelEncoder\npredicted_disease = label_encoder.inverse_transform([predicted_label_index])\n\nprint(predicted_disease[0])  # Output the predicted disease name\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T10:01:52.848136Z","iopub.execute_input":"2025-02-25T10:01:52.848446Z","iopub.status.idle":"2025-02-25T10:01:53.269035Z","shell.execute_reply.started":"2025-02-25T10:01:52.848420Z","shell.execute_reply":"2025-02-25T10:01:53.268220Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"pneumoconiosis\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import optuna\nimport os\nfrom transformers import TrainingArguments, Trainer\n\n# Global variable to track the best model\nbest_model = None\nbest_tokenizer = None\nbest_loss = float(\"inf\")  # Initialize best loss as infinity\n\ndef objective(trial):\n    global best_model, best_tokenizer, best_loss\n\n    # Define hyperparameter search space\n    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-5)\n    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n    epochs = trial.suggest_int(\"epochs\", 3, 10)\n\n    training_args = TrainingArguments(\n        output_dir=\"./pubmedbert_optuna\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        learning_rate=learning_rate,\n        num_train_epochs=epochs,\n        weight_decay=0.01,\n        save_total_limit=2,\n        load_best_model_at_end=True  # âœ… Ensures best model is used\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        tokenizer=tokenizer\n    )\n\n    trainer.train()\n    eval_results = trainer.evaluate()\n    eval_loss = eval_results[\"eval_loss\"]\n\n    # âœ… Store only the best model\n    if eval_loss < best_loss:\n        best_loss = eval_loss\n        best_model = model\n        best_tokenizer = tokenizer\n        print(f\"âœ… New best model found! Loss: {best_loss}\")\n\n    return eval_loss  # Minimize loss\n\n# Run Optuna tuning\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=2)\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(\"Best Hyperparameters:\", best_params)\n\n# âœ… Save only the best model after tuning completes\nbest_model_path = f\"best_model_lr{best_params['learning_rate']}_bs{best_params['batch_size']}_epochs{best_params['epochs']}\"\nos.makedirs(best_model_path, exist_ok=True)\nbest_model.save_pretrained(best_model_path)\nbest_tokenizer.save_pretrained(best_model_path)\n\nprint(f\"âœ… Best model saved at: {best_model_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T06:52:48.208969Z","iopub.execute_input":"2025-02-25T06:52:48.209299Z","iopub.status.idle":"2025-02-25T07:39:19.420332Z","shell.execute_reply.started":"2025-02-25T06:52:48.209271Z","shell.execute_reply":"2025-02-25T07:39:19.419556Z"}},"outputs":[{"name":"stderr","text":"[I 2025-02-25 06:52:48,214] A new study created in memory with name: no-name-42aa48e8-83c3-4e34-85e9-86ba3b712f6c\n<ipython-input-11-34b2b313e907>:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-5)\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-11-34b2b313e907>:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdhrupalpatidar1313\u001b[0m (\u001b[33mdhrupalpatidar1313-wappnet-systems-pvt-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250225_065248-gbn5cn5q</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dhrupalpatidar1313-wappnet-systems-pvt-/huggingface/runs/gbn5cn5q' target=\"_blank\">./pubmedbert_optuna</a></strong> to <a href='https://wandb.ai/dhrupalpatidar1313-wappnet-systems-pvt-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dhrupalpatidar1313-wappnet-systems-pvt-/huggingface' target=\"_blank\">https://wandb.ai/dhrupalpatidar1313-wappnet-systems-pvt-/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dhrupalpatidar1313-wappnet-systems-pvt-/huggingface/runs/gbn5cn5q' target=\"_blank\">https://wandb.ai/dhrupalpatidar1313-wappnet-systems-pvt-/huggingface/runs/gbn5cn5q</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7000' max='7000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7000/7000 28:35, Epoch 7/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>4.632900</td>\n      <td>3.649493</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.429400</td>\n      <td>1.699323</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.076100</td>\n      <td>0.654263</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.437300</td>\n      <td>0.247509</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.198900</td>\n      <td>0.114712</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.116200</td>\n      <td>0.070636</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.090100</td>\n      <td>0.059622</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:18]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-02-25 07:21:53,880] Trial 0 finished with value: 0.05962178111076355 and parameters: {'learning_rate': 2.6331868576418965e-05, 'batch_size': 8, 'epochs': 7}. Best is trial 0 with value: 0.05962178111076355.\n","output_type":"stream"},{"name":"stdout","text":"âœ… New best model found! Loss: 0.05962178111076355\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-11-34b2b313e907>:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-5)\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-11-34b2b313e907>:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3000/3000 17:11, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.066200</td>\n      <td>0.026688</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.030300</td>\n      <td>0.015494</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.019400</td>\n      <td>0.011082</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.014500</td>\n      <td>0.009019</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.012000</td>\n      <td>0.007954</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.010900</td>\n      <td>0.007615</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 00:12]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-02-25 07:39:18,375] Trial 1 finished with value: 0.0076147764921188354 and parameters: {'learning_rate': 2.4432297149345087e-05, 'batch_size': 16, 'epochs': 6}. Best is trial 1 with value: 0.0076147764921188354.\n","output_type":"stream"},{"name":"stdout","text":"âœ… New best model found! Loss: 0.0076147764921188354\nBest Hyperparameters: {'learning_rate': 2.4432297149345087e-05, 'batch_size': 16, 'epochs': 6}\nâœ… Best model saved at: best_model_lr2.4432297149345087e-05_bs16_epochs6\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T10:03:32.286316Z","iopub.execute_input":"2025-02-25T10:03:32.286641Z","iopub.status.idle":"2025-02-25T10:05:55.043545Z","shell.execute_reply.started":"2025-02-25T10:03:32.286613Z","shell.execute_reply":"2025-02-25T10:05:55.042556Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/logs/ (stored 0%)\n  adding: kaggle/working/logs/events.out.tfevents.1740477675.a6f9d4425b2e.31.1 (deflated 25%)\n  adding: kaggle/working/logs/events.out.tfevents.1740475581.a6f9d4425b2e.31.0 (deflated 75%)\n  adding: kaggle/working/label_encoder.pkl (deflated 59%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n  adding: kaggle/working/wandb/ (stored 0%)\n  adding: kaggle/working/wandb/debug-internal.log (deflated 75%)\n  adding: kaggle/working/wandb/debug.log (deflated 71%)\n  adding: kaggle/working/wandb/run-20250225_092627-yxlcqn81/ (stored 0%)\n  adding: kaggle/working/wandb/run-20250225_092627-yxlcqn81/run-yxlcqn81.wandb (deflated 80%)\n  adding: kaggle/working/wandb/run-20250225_092627-yxlcqn81/logs/ (stored 0%)\n  adding: kaggle/working/wandb/run-20250225_092627-yxlcqn81/logs/debug-internal.log (deflated 75%)\n  adding: kaggle/working/wandb/run-20250225_092627-yxlcqn81/logs/debug.log (deflated 71%)\n  adding: kaggle/working/wandb/run-20250225_092627-yxlcqn81/logs/debug-core.log (deflated 58%)\n  adding: kaggle/working/wandb/run-20250225_092627-yxlcqn81/files/ (stored 0%)\n  adding: kaggle/working/wandb/run-20250225_092627-yxlcqn81/files/wandb-metadata.json (deflated 54%)\n  adding: kaggle/working/wandb/run-20250225_092627-yxlcqn81/files/requirements.txt (deflated 56%)\n  adding: kaggle/working/wandb/run-20250225_092627-yxlcqn81/files/output.log (deflated 89%)\n  adding: kaggle/working/wandb/run-20250225_092627-yxlcqn81/tmp/ (stored 0%)\n  adding: kaggle/working/wandb/run-20250225_092627-yxlcqn81/tmp/code/ (stored 0%)\n  adding: kaggle/working/wandb/latest-run/ (stored 0%)\n  adding: kaggle/working/wandb/latest-run/run-yxlcqn81.wandb (deflated 80%)\n  adding: kaggle/working/wandb/latest-run/logs/ (stored 0%)\n  adding: kaggle/working/wandb/latest-run/logs/debug-internal.log (deflated 75%)\n  adding: kaggle/working/wandb/latest-run/logs/debug.log (deflated 71%)\n  adding: kaggle/working/wandb/latest-run/logs/debug-core.log (deflated 58%)\n  adding: kaggle/working/wandb/latest-run/files/ (stored 0%)\n  adding: kaggle/working/wandb/latest-run/files/wandb-metadata.json (deflated 54%)\n  adding: kaggle/working/wandb/latest-run/files/requirements.txt (deflated 56%)\n  adding: kaggle/working/wandb/latest-run/files/output.log (deflated 89%)\n  adding: kaggle/working/wandb/latest-run/tmp/ (stored 0%)\n  adding: kaggle/working/wandb/latest-run/tmp/code/ (stored 0%)\n  adding: kaggle/working/fine_tuned_pubmedbert/ (stored 0%)\n  adding: kaggle/working/fine_tuned_pubmedbert/tokenizer.json (deflated 71%)\n  adding: kaggle/working/fine_tuned_pubmedbert/tokenizer_config.json (deflated 74%)\n  adding: kaggle/working/fine_tuned_pubmedbert/model.safetensors (deflated 7%)\n  adding: kaggle/working/fine_tuned_pubmedbert/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/fine_tuned_pubmedbert/config.json (deflated 78%)\n  adding: kaggle/working/fine_tuned_pubmedbert/vocab.txt (deflated 54%)\n  adding: kaggle/working/pubmedbert_disease_prediction/ (stored 0%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-4500/ (stored 0%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-4500/tokenizer.json (deflated 71%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-4500/trainer_state.json (deflated 73%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-4500/tokenizer_config.json (deflated 74%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-4500/model.safetensors (deflated 7%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-4500/optimizer.pt (deflated 28%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-4500/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-4500/scheduler.pt (deflated 56%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-4500/config.json (deflated 78%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-4500/rng_state.pth (deflated 25%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-4500/training_args.bin (deflated 51%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-4500/vocab.txt (deflated 54%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-5000/ (stored 0%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-5000/tokenizer.json (deflated 71%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-5000/trainer_state.json (deflated 74%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-5000/tokenizer_config.json (deflated 74%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-5000/model.safetensors (deflated 7%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-5000/optimizer.pt (deflated 28%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-5000/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-5000/scheduler.pt (deflated 56%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-5000/config.json (deflated 78%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-5000/rng_state.pth (deflated 25%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-5000/training_args.bin (deflated 51%)\n  adding: kaggle/working/pubmedbert_disease_prediction/checkpoint-5000/vocab.txt (deflated 54%)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'file.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T10:05:55.045295Z","iopub.execute_input":"2025-02-25T10:05:55.045609Z","iopub.status.idle":"2025-02-25T10:05:55.051897Z","shell.execute_reply.started":"2025-02-25T10:05:55.045586Z","shell.execute_reply":"2025-02-25T10:05:55.051060Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/file.zip","text/html":"<a href='file.zip' target='_blank'>file.zip</a><br>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}